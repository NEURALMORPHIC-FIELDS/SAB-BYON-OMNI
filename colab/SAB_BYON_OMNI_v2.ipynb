{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<div align=\"center\">\n\n# SAB + BYON-OMNI v2.0\n## Unified Consciousness System\n### Google Colab Training & Evaluation Pipeline\n\n**40 Capabilities** | OmniAGI Nexus Model | Industrial LLM Benchmarks\n\n</div>\n\n---\n\n### Sections:\n1. **File System** - Project structure creation\n2. **Dependencies** - Install all required packages\n3. **Source Code** - Paste your code here\n4. **Training** - Full training pipeline\n5. **Industrial Benchmarks** - Standard LLM evaluation with scores"
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# LOGO DISPLAY - Upload logo.png to Colab first (from the colab/ folder)\n# ============================================================================\nfrom IPython.display import display, Image, HTML\nimport os\n\n# Try to find logo in common locations\nlogo_paths = [\n    '/content/logo.png',              # If uploaded directly to Colab\n    '/content/SAB-BYON-OMNI/logo.png', # If in project root\n    'logo.png',                        # Current directory\n]\n\nlogo_found = False\nfor path in logo_paths:\n    if os.path.exists(path):\n        display(HTML(f'''\n        <div style=\"text-align: center; padding: 20px;\">\n            <img src=\"data:image/png;base64,{__import__(\"base64\").b64encode(open(path,\"rb\").read()).decode()}\"\n                 width=\"400\" style=\"border-radius: 12px; box-shadow: 0 4px 12px rgba(0,0,0,0.3);\"/>\n            <h3 style=\"color: #2196F3; margin-top: 15px;\">SAB-BYON-OMNI-AI v2.0</h3>\n            <p style=\"color: #666;\">Unified Consciousness System | 40 Capabilities</p>\n        </div>\n        '''))\n        logo_found = True\n        break\n\nif not logo_found:\n    # Upload prompt\n    from google.colab import files\n    print(\"Upload logo.png (from the colab/ folder):\")\n    uploaded = files.upload()\n    if 'logo.png' in uploaded:\n        with open('/content/logo.png', 'wb') as f:\n            f.write(uploaded['logo.png'])\n        display(HTML(f'''\n        <div style=\"text-align: center; padding: 20px;\">\n            <img src=\"data:image/png;base64,{__import__(\"base64\").b64encode(uploaded[\"logo.png\"]).decode()}\"\n                 width=\"400\" style=\"border-radius: 12px; box-shadow: 0 4px 12px rgba(0,0,0,0.3);\"/>\n            <h3 style=\"color: #2196F3; margin-top: 15px;\">SAB-BYON-OMNI-AI v2.0</h3>\n            <p style=\"color: #666;\">Unified Consciousness System | 40 Capabilities</p>\n        </div>\n        '''))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 1: File System Setup\n",
    "Creates the complete project directory structure on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os, shutil\n\n# ============================================================================\n# SECTION 1: FILE SYSTEM - Project Structure\n# ============================================================================\n\nPROJECT_ROOT = '/content/SAB-BYON-OMNI'\n\n# Complete directory tree\ndirectories = [\n    f'{PROJECT_ROOT}/sab_byon_omni',\n    f'{PROJECT_ROOT}/sab_byon_omni/quantifiers',\n    f'{PROJECT_ROOT}/sab_byon_omni/evolution',\n    f'{PROJECT_ROOT}/sab_byon_omni/memory',\n    f'{PROJECT_ROOT}/sab_byon_omni/agents',\n    f'{PROJECT_ROOT}/sab_byon_omni/cognitive',\n    f'{PROJECT_ROOT}/sab_byon_omni/consciousness',\n    f'{PROJECT_ROOT}/sab_byon_omni/model',\n    f'{PROJECT_ROOT}/sab_byon_omni/training',\n    f'{PROJECT_ROOT}/sab_byon_omni/core',\n    f'{PROJECT_ROOT}/configs',\n    f'{PROJECT_ROOT}/tests',\n    f'{PROJECT_ROOT}/scripts',\n    f'{PROJECT_ROOT}/checkpoints',\n    f'{PROJECT_ROOT}/logs',\n    f'{PROJECT_ROOT}/results',\n]\n\nfor d in directories:\n    os.makedirs(d, exist_ok=True)\n    print(f'  [OK] {d}')\n\n# Copy logo to project root\nlogo_sources = ['/content/logo.png', 'logo.png']\nfor src in logo_sources:\n    if os.path.exists(src):\n        shutil.copy2(src, f'{PROJECT_ROOT}/logo.png')\n        print(f'  [LOGO] Copied logo.png to project root')\n        break\n\n# Create all __init__.py files\ninit_dirs = [\n    'sab_byon_omni',\n    'sab_byon_omni/quantifiers',\n    'sab_byon_omni/evolution',\n    'sab_byon_omni/memory',\n    'sab_byon_omni/agents',\n    'sab_byon_omni/cognitive',\n    'sab_byon_omni/consciousness',\n    'sab_byon_omni/model',\n    'sab_byon_omni/training',\n    'sab_byon_omni/core',\n]\n\nfor d in init_dirs:\n    init_path = os.path.join(PROJECT_ROOT, d, '__init__.py')\n    if not os.path.exists(init_path):\n        with open(init_path, 'w') as f:\n            f.write('# -*- coding: utf-8 -*-\\n')\n        print(f'  [INIT] {init_path}')\n\n# Write default.yaml config\nconfig_yaml = '''# SAB + BYON-OMNI v2.0 Default Configuration\nmodel:\n  vocab_size: 50000\n  hidden_size: 4096\n  num_attention_heads: 64\n  num_hidden_layers: 36\n  intermediate_size: 16384\n  max_position_embeddings: 4096\n  initializer_range: 0.02\n\nmodel_lightweight:\n  vocab_size: 50000\n  hidden_size: 768\n  num_attention_heads: 12\n  num_hidden_layers: 6\n  intermediate_size: 3072\n  max_position_embeddings: 2048\n\nfragmergent:\n  alpha: 0.02\n  lambda: 0.2\n  omega: 2.0\n\ntdfc:\n  grid_size: 32\n  diffusion_coeff: 0.1\n  pde_steps: 50\n  dt: 0.01\n  momentum: 0.9\n  virtue_names:\n    - stoicism\n    - discernment\n    - philosophy\n    - empathy\n    - curiosity\n    - humility\n    - creativity\n    - reflexivity\n    - truthlove\n    - holographic\n\nconsciousness:\n  unified_weights:\n    triadic: 0.25\n    PLV: 0.20\n    CFC: 0.15\n    Phi: 0.15\n    spectral: 0.15\n    fragmergent: 0.10\n\ntraining:\n  epochs: 3\n  batch_size: 4\n  gradient_accumulation_steps: 16\n  learning_rate: 2.0e-5\n  weight_decay: 0.01\n  max_grad_norm: 1.0\n  seq_len: 1024\n  num_samples: 5000\n  num_workers: 2\n  log_every_batches: 5\n\nmemory:\n  holographic_shape: [16, 16, 16, 16]\n  evolutionary_layers:\n    immediate: 50\n    working: 100\n    persistent: 200\n    archetypal: 300\n  compression_target: 0.1\n\nagents:\n  rl:\n    state_size: 100\n    action_size: 5\n    alpha: 0.1\n    gamma: 0.6\n    epsilon: 0.1\n  fragmergent:\n    synergy_level: 0.3\n  memory_manager:\n    short_size: 2000\n'''\n\nwith open(f'{PROJECT_ROOT}/configs/default.yaml', 'w') as f:\n    f.write(config_yaml)\nprint(f'  [CONFIG] configs/default.yaml')\n\n# Add project to Python path\nimport sys\nif PROJECT_ROOT not in sys.path:\n    sys.path.insert(0, PROJECT_ROOT)\n\n# Verify structure\nprint('\\n' + '='*60)\nprint('FILE SYSTEM STRUCTURE:')\nprint('='*60)\nfor root, dirs, files in os.walk(PROJECT_ROOT):\n    level = root.replace(PROJECT_ROOT, '').count(os.sep)\n    indent = ' ' * 2 * level\n    print(f'{indent}{os.path.basename(root)}/')\n    subindent = ' ' * 2 * (level + 1)\n    for file in files:\n        print(f'{subindent}{file}')\n\nprint('\\n[OK] File system ready!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 2: Dependencies\n",
    "Installs all required packages for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: DEPENDENCIES\n",
    "# ============================================================================\n",
    "\n",
    "!pip install -q torch>=2.0.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q transformers>=4.30.0\n",
    "!pip install -q datasets>=2.14.0\n",
    "!pip install -q accelerate>=0.21.0\n",
    "!pip install -q scipy>=1.11.0\n",
    "!pip install -q psutil>=5.9.0\n",
    "!pip install -q matplotlib>=3.7.0\n",
    "!pip install -q pandas>=2.0.0\n",
    "!pip install -q seaborn>=0.12.0\n",
    "!pip install -q numpy>=1.24.0\n",
    "!pip install -q pyyaml\n",
    "!pip install -q sentencepiece\n",
    "!pip install -q tokenizers\n",
    "\n",
    "# Benchmark dependencies\n",
    "!pip install -q lm-eval>=0.4.0        # EleutherAI LM Evaluation Harness\n",
    "!pip install -q rouge-score            # ROUGE metrics\n",
    "!pip install -q nltk                   # NLP toolkit\n",
    "!pip install -q sacrebleu              # BLEU scoring\n",
    "!pip install -q scikit-learn           # ML metrics\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "# Verify GPU\n",
    "import torch\n",
    "print('\\n' + '='*60)\n",
    "print('ENVIRONMENT CHECK:')\n",
    "print('='*60)\n",
    "print(f'  PyTorch: {torch.__version__}')\n",
    "print(f'  CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'  GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'  VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB')\n",
    "\n",
    "import transformers\n",
    "print(f'  Transformers: {transformers.__version__}')\n",
    "\n",
    "try:\n",
    "    import lm_eval\n",
    "    print(f'  LM-Eval Harness: {lm_eval.__version__}')\n",
    "except:\n",
    "    print('  LM-Eval Harness: installed')\n",
    "\n",
    "print('\\n[OK] All dependencies installed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 3: Source Code\n",
    "**Paste your source files here.** Each cell corresponds to one module.\n",
    "\n",
    "Run each cell after pasting to write the file to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 3: SOURCE CODE - Helper function\n",
    "# ============================================================================\n",
    "\n",
    "PROJECT_ROOT = '/content/SAB-BYON-OMNI'\n",
    "\n",
    "def write_source(relative_path, code):\n",
    "    \"\"\"Write source code to the project tree.\"\"\"\n",
    "    full_path = os.path.join(PROJECT_ROOT, relative_path)\n",
    "    os.makedirs(os.path.dirname(full_path), exist_ok=True)\n",
    "    with open(full_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(code)\n",
    "    lines = code.count('\\n') + 1\n",
    "    print(f'  [WRITTEN] {relative_path} ({lines} lines)')\n",
    "\n",
    "print('write_source() helper ready. Use it in the cells below.')\n",
    "print('Example: write_source(\"sab_byon_omni/config.py\", YOUR_CODE)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.1: sab_byon_omni/config.py ---\n",
    "# PASTE YOUR CODE BELOW between the triple quotes, then run this cell\n",
    "\n",
    "write_source('sab_byon_omni/config.py', r'''\n",
    "# PASTE sab_byon_omni/config.py HERE\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.2: sab_byon_omni/__init__.py ---\n",
    "\n",
    "write_source('sab_byon_omni/__init__.py', r'''\n",
    "# PASTE sab_byon_omni/__init__.py HERE\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.3: QUANTIFIERS ---\n",
    "# Run this cell for each quantifier file\n",
    "\n",
    "# write_source('sab_byon_omni/quantifiers/base_quantifier.py', r'''...''')\n",
    "# write_source('sab_byon_omni/quantifiers/statistical_quantifier.py', r'''...''')\n",
    "# write_source('sab_byon_omni/quantifiers/entropy_quantifier.py', r'''...''')\n",
    "# write_source('sab_byon_omni/quantifiers/cryptographic_prng.py', r'''...''')\n",
    "# write_source('sab_byon_omni/quantifiers/reasoning_quantifier.py', r'''...''')\n",
    "# write_source('sab_byon_omni/quantifiers/memory_relevance_quantifier.py', r'''...''')\n",
    "# write_source('sab_byon_omni/quantifiers/decision_confidence_quantifier.py', r'''...''')\n",
    "# write_source('sab_byon_omni/quantifiers/quantification_result.py', r'''...''')\n",
    "# write_source('sab_byon_omni/quantifiers/__init__.py', r'''...''')\n",
    "\n",
    "print('Uncomment and paste each file, then run.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.4: EVOLUTION ---\n",
    "\n",
    "# write_source('sab_byon_omni/evolution/frag_param.py', r'''...''')\n",
    "# write_source('sab_byon_omni/evolution/metrics_module.py', r'''...''')\n",
    "# write_source('sab_byon_omni/evolution/pathway_evolution.py', r'''...''')\n",
    "# write_source('sab_byon_omni/evolution/dim1_universal.py', r'''...''')\n",
    "# write_source('sab_byon_omni/evolution/__init__.py', r'''...''')\n",
    "\n",
    "print('Uncomment and paste each file, then run.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.5: MEMORY ---\n",
    "\n",
    "# write_source('sab_byon_omni/memory/memory_chunk.py', r'''...''')\n",
    "# write_source('sab_byon_omni/memory/fragmergent_memory.py', r'''...''')\n",
    "# write_source('sab_byon_omni/memory/holographic_memory.py', r'''...''')\n",
    "# write_source('sab_byon_omni/memory/conversation_manager.py', r'''...''')\n",
    "# write_source('sab_byon_omni/memory/__init__.py', r'''...''')\n",
    "\n",
    "print('Uncomment and paste each file, then run.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.6: AGENTS ---\n",
    "\n",
    "# write_source('sab_byon_omni/agents/base_agent.py', r'''...''')\n",
    "# write_source('sab_byon_omni/agents/rl_agent.py', r'''...''')\n",
    "# write_source('sab_byon_omni/agents/fragmergent_agent.py', r'''...''')\n",
    "# write_source('sab_byon_omni/agents/memory_agent.py', r'''...''')\n",
    "# write_source('sab_byon_omni/agents/multi_agent_cortex.py', r'''...''')\n",
    "# write_source('sab_byon_omni/agents/__init__.py', r'''...''')\n",
    "\n",
    "print('Uncomment and paste each file, then run.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.7: COGNITIVE ---\n",
    "\n",
    "# write_source('sab_byon_omni/cognitive/fisher_geometry.py', r'''...''')\n",
    "# write_source('sab_byon_omni/cognitive/info_density_field.py', r'''...''')\n",
    "# write_source('sab_byon_omni/cognitive/semantic_photon.py', r'''...''')\n",
    "# write_source('sab_byon_omni/cognitive/duei_framework.py', r'''...''')\n",
    "# write_source('sab_byon_omni/cognitive/personality.py', r'''...''')\n",
    "# write_source('sab_byon_omni/cognitive/__init__.py', r'''...''')\n",
    "\n",
    "print('Uncomment and paste each file, then run.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.8: CONSCIOUSNESS ---\n",
    "\n",
    "# write_source('sab_byon_omni/consciousness/triadic_state.py', r'''...''')\n",
    "# write_source('sab_byon_omni/consciousness/tdfc_engine.py', r'''...''')\n",
    "# write_source('sab_byon_omni/consciousness/godel_engine.py', r'''...''')\n",
    "# write_source('sab_byon_omni/consciousness/icf.py', r'''...''')\n",
    "# write_source('sab_byon_omni/consciousness/fragmergent_engine.py', r'''...''')\n",
    "# write_source('sab_byon_omni/consciousness/time_emergence.py', r'''...''')\n",
    "# write_source('sab_byon_omni/consciousness/zeta_resonance.py', r'''...''')\n",
    "# write_source('sab_byon_omni/consciousness/emergence_detector.py', r'''...''')\n",
    "# write_source('sab_byon_omni/consciousness/__init__.py', r'''...''')\n",
    "\n",
    "print('Uncomment and paste each file, then run.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.9: MODEL ---\n",
    "\n",
    "# write_source('sab_byon_omni/model/config.py', r'''...''')\n",
    "# write_source('sab_byon_omni/model/omni_agi_nexus.py', r'''...''')\n",
    "# write_source('sab_byon_omni/model/__init__.py', r'''...''')\n",
    "\n",
    "print('Uncomment and paste each file, then run.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.10: TRAINING ---\n",
    "\n",
    "# write_source('sab_byon_omni/training/train_3b.py', r'''...''')\n",
    "# write_source('sab_byon_omni/training/__init__.py', r'''...''')\n",
    "\n",
    "print('Uncomment and paste each file, then run.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.11: CORE ---\n",
    "\n",
    "# write_source('sab_byon_omni/core/sab_transcendent.py', r'''...''')\n",
    "# write_source('sab_byon_omni/core/__init__.py', r'''...''')\n",
    "\n",
    "print('Uncomment and paste each file, then run.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.12: VERIFY ALL IMPORTS ---\n",
    "\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = '/content/SAB-BYON-OMNI'\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# Force reimport\n",
    "for mod_name in list(sys.modules.keys()):\n",
    "    if 'sab_byon_omni' in mod_name:\n",
    "        del sys.modules[mod_name]\n",
    "\n",
    "try:\n",
    "    from sab_byon_omni import SABTranscendentV2\n",
    "    print('[OK] SABTranscendentV2 imported successfully')\n",
    "    print('[OK] All source code is correctly placed')\n",
    "except Exception as e:\n",
    "    print(f'[ERROR] Import failed: {e}')\n",
    "    print('Check that all source files are pasted correctly above.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 4: Training\n",
    "Full training pipeline with mixed precision, gradient accumulation, and live monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 4: TRAINING PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "import os, sys, time, json, gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "PROJECT_ROOT = '/content/SAB-BYON-OMNI'\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# Force clean reimport\n",
    "for mod_name in list(sys.modules.keys()):\n",
    "    if 'sab_byon_omni' in mod_name:\n",
    "        del sys.modules[mod_name]\n",
    "\n",
    "from sab_byon_omni.core.sab_transcendent import SABTranscendentV2\n",
    "from sab_byon_omni.model.config import MultimodalConsciousnessDataset\n",
    "\n",
    "# ---- CONFIG ----\n",
    "TRAIN_CONFIG = {\n",
    "    'epochs': 3,\n",
    "    'batch_size': 4,\n",
    "    'grad_accum': 16,\n",
    "    'learning_rate': 2e-5,\n",
    "    'weight_decay': 0.01,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'seq_len': 1024,\n",
    "    'num_samples': 5000,\n",
    "    'num_workers': 2,\n",
    "    'log_every': 5,\n",
    "    'save_every_epoch': True,\n",
    "}\n",
    "\n",
    "print('='*70)\n",
    "print('SAB + BYON-OMNI v2.0 - TRAINING PIPELINE')\n",
    "print('='*70)\n",
    "for k, v in TRAIN_CONFIG.items():\n",
    "    print(f'  {k}: {v}')\n",
    "\n",
    "# ---- INITIALIZE SYSTEM ----\n",
    "print('\\nInitializing SAB Transcendent v2.0...')\n",
    "sab = SABTranscendentV2()\n",
    "model = sab.llm.model\n",
    "device = sab.llm.device\n",
    "\n",
    "if model is None:\n",
    "    raise RuntimeError('Model not initialized. Check HuggingFace imports.')\n",
    "\n",
    "model.to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'\\nModel loaded on {device}')\n",
    "print(f'  Total parameters: {total_params:,}')\n",
    "print(f'  Trainable parameters: {trainable_params:,}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'  VRAM allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB')\n",
    "    print(f'  VRAM reserved:  {torch.cuda.memory_reserved()/1e9:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- DATASET & DATALOADER ----\n",
    "cfg = TRAIN_CONFIG\n",
    "\n",
    "dataset = MultimodalConsciousnessDataset(\n",
    "    num_samples=cfg['num_samples'],\n",
    "    max_len=cfg['seq_len']\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=cfg['batch_size'],\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=cfg['num_workers'],\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "total_batches = len(dataloader)\n",
    "steps_per_epoch = total_batches // cfg['grad_accum']\n",
    "total_steps = steps_per_epoch * cfg['epochs']\n",
    "\n",
    "print(f'Dataset: {len(dataset)} samples')\n",
    "print(f'Batches/epoch: {total_batches}')\n",
    "print(f'Optimizer steps/epoch: {steps_per_epoch}')\n",
    "print(f'Total optimizer steps: {total_steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- TRAINING LOOP ----\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=cfg['learning_rate'],\n",
    "    weight_decay=cfg['weight_decay']\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'batch_loss': [],\n",
    "    'step_loss': [],\n",
    "    'epoch_loss': [],\n",
    "    'learning_rates': [],\n",
    "    'vram_usage': [],\n",
    "    'step_times': [],\n",
    "}\n",
    "\n",
    "model.train()\n",
    "global_step = 0\n",
    "best_loss = float('inf')\n",
    "t_start = time.perf_counter()\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('STARTING TRAINING')\n",
    "print('='*70)\n",
    "\n",
    "for epoch in range(cfg['epochs']):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_start = time.perf_counter()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(f'\\n{\"=\"*70}')\n",
    "    print(f'EPOCH {epoch+1}/{cfg[\"epochs\"]}')\n",
    "    print(f'{\"=\"*70}')\n",
    "\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        step_t0 = time.perf_counter()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "        attention_mask = batch.get('attention_mask')\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.to(device, non_blocking=True)\n",
    "        labels = batch['labels'].to(device, non_blocking=True)\n",
    "\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs['logits']\n",
    "            # Causal LM shift\n",
    "            shift_logits = logits[..., :-1, :].contiguous().view(-1, logits.size(-1))\n",
    "            shift_labels = labels[..., 1:].contiguous().view(-1)\n",
    "            loss = criterion(shift_logits, shift_labels) / cfg['grad_accum']\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        batch_loss_val = loss.item() * cfg['grad_accum']\n",
    "        epoch_loss += batch_loss_val\n",
    "        history['batch_loss'].append(batch_loss_val)\n",
    "\n",
    "        # Optimizer step\n",
    "        if (idx + 1) % cfg['grad_accum'] == 0 or (idx + 1) == total_batches:\n",
    "            scaler.unscale_(optimizer)\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), cfg['max_grad_norm'])\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "            step_time = time.perf_counter() - step_t0\n",
    "            history['step_loss'].append(batch_loss_val)\n",
    "            history['step_times'].append(step_time)\n",
    "            history['learning_rates'].append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                vram = torch.cuda.memory_allocated() / 1e9\n",
    "                history['vram_usage'].append(vram)\n",
    "            else:\n",
    "                vram = 0\n",
    "\n",
    "            elapsed = time.perf_counter() - t_start\n",
    "            print(f'  Step {global_step:04d}/{total_steps} | '\n",
    "                  f'batch {idx+1}/{total_batches} | '\n",
    "                  f'loss={batch_loss_val:.4f} | '\n",
    "                  f'grad_norm={grad_norm:.3f} | '\n",
    "                  f'VRAM={vram:.1f}GB | '\n",
    "                  f'{elapsed:.0f}s')\n",
    "\n",
    "        elif (idx + 1) % cfg['log_every'] == 0:\n",
    "            pct = (idx + 1) / total_batches * 100\n",
    "            print(f'    batch {idx+1}/{total_batches} ({pct:.0f}%) | loss={batch_loss_val:.4f}')\n",
    "\n",
    "    # Epoch summary\n",
    "    avg_loss = epoch_loss / total_batches\n",
    "    epoch_time = time.perf_counter() - epoch_start\n",
    "    history['epoch_loss'].append(avg_loss)\n",
    "\n",
    "    print(f'\\n  Epoch {epoch+1} Summary:')\n",
    "    print(f'    Avg loss: {avg_loss:.4f}')\n",
    "    print(f'    Time: {epoch_time:.0f}s')\n",
    "    print(f'    Samples/sec: {len(dataset)/epoch_time:.1f}')\n",
    "\n",
    "    # Save checkpoint\n",
    "    if cfg['save_every_epoch']:\n",
    "        ckpt_path = f'{PROJECT_ROOT}/checkpoints/epoch_{epoch+1}.pt'\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "            'global_step': global_step,\n",
    "            'config': TRAIN_CONFIG,\n",
    "        }, ckpt_path)\n",
    "        print(f'    Checkpoint saved: {ckpt_path}')\n",
    "\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        best_path = f'{PROJECT_ROOT}/checkpoints/best_model.pt'\n",
    "        torch.save(model.state_dict(), best_path)\n",
    "        print(f'    Best model saved: {best_path}')\n",
    "\n",
    "total_time = time.perf_counter() - t_start\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print(f'TRAINING COMPLETE')\n",
    "print(f'  Total steps: {global_step}')\n",
    "print(f'  Total time: {total_time:.0f}s ({total_time/60:.1f}min)')\n",
    "print(f'  Best loss: {best_loss:.4f}')\n",
    "print(f'{\"=\"*70}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- TRAINING CURVES ----\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('SAB + BYON-OMNI v2.0 - Training Metrics', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Loss per batch\n",
    "axes[0,0].plot(history['batch_loss'], alpha=0.3, color='blue', linewidth=0.5)\n",
    "# Smoothed\n",
    "if len(history['batch_loss']) > 20:\n",
    "    window = min(50, len(history['batch_loss'])//5)\n",
    "    smoothed = np.convolve(history['batch_loss'], np.ones(window)/window, mode='valid')\n",
    "    axes[0,0].plot(range(window-1, window-1+len(smoothed)), smoothed, color='red', linewidth=2)\n",
    "axes[0,0].set_title('Batch Loss')\n",
    "axes[0,0].set_xlabel('Batch')\n",
    "axes[0,0].set_ylabel('Loss')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss per optimizer step\n",
    "axes[0,1].plot(history['step_loss'], color='green', linewidth=1.5)\n",
    "axes[0,1].set_title('Optimizer Step Loss')\n",
    "axes[0,1].set_xlabel('Step')\n",
    "axes[0,1].set_ylabel('Loss')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Epoch loss\n",
    "axes[1,0].bar(range(1, len(history['epoch_loss'])+1), history['epoch_loss'], color='orange')\n",
    "axes[1,0].set_title('Epoch Average Loss')\n",
    "axes[1,0].set_xlabel('Epoch')\n",
    "axes[1,0].set_ylabel('Avg Loss')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# VRAM usage\n",
    "if history['vram_usage']:\n",
    "    axes[1,1].plot(history['vram_usage'], color='purple', linewidth=1.5)\n",
    "    axes[1,1].set_title('VRAM Usage (GB)')\n",
    "    axes[1,1].set_xlabel('Step')\n",
    "    axes[1,1].set_ylabel('GB')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1,1].text(0.5, 0.5, 'No GPU', ha='center', va='center', fontsize=14)\n",
    "    axes[1,1].set_title('VRAM Usage')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{PROJECT_ROOT}/results/training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Saved: {PROJECT_ROOT}/results/training_curves.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 5: Industrial LLM Benchmarks & Scoring\n",
    "\n",
    "Standard evaluation suite matching how production LLMs are tested:\n",
    "\n",
    "| Benchmark | What it measures | Industry standard |\n",
    "|-----------|-----------------|-------------------|\n",
    "| **Perplexity** | Language modeling quality | Lower = better |\n",
    "| **MMLU-style** | Knowledge & reasoning (multiple choice) | GPT-4: ~86% |\n",
    "| **HellaSwag-style** | Commonsense reasoning | GPT-4: ~95% |\n",
    "| **ARC-style** | Science reasoning | GPT-4: ~96% |\n",
    "| **TruthfulQA-style** | Truthfulness & factuality | GPT-4: ~59% |\n",
    "| **Coherence** | Output consistency & semantic quality | Higher = better |\n",
    "| **Generation Speed** | Tokens per second throughput | Higher = better |\n",
    "| **Memory Efficiency** | VRAM usage & parameter efficiency | Lower = better |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: INDUSTRIAL LLM BENCHMARKS\n",
    "# ============================================================================\n",
    "\n",
    "import os, sys, time, json, math, gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "PROJECT_ROOT = '/content/SAB-BYON-OMNI'\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print('='*70)\n",
    "print('SAB + BYON-OMNI v2.0 - INDUSTRIAL LLM BENCHMARK SUITE')\n",
    "print('='*70)\n",
    "print('Standard evaluation matching GPT-4 / LLaMA / Mistral test protocols')\n",
    "print()\n",
    "\n",
    "# ---- Load model ----\n",
    "model.eval()\n",
    "device = next(model.parameters()).device\n",
    "vocab_size = model.config.vocab_size\n",
    "\n",
    "print(f'Model: OmniAGI Nexus on {device}')\n",
    "print(f'Parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
    "print(f'Vocab size: {vocab_size}')\n",
    "\n",
    "# Build tokenizer mapping (char-level as in the model)\n",
    "vocab_list = ['<PAD>'] + [chr(i) for i in range(32, 127)]\n",
    "c2i = {c: i for i, c in enumerate(vocab_list)}\n",
    "i2c = {i: c for c, i in c2i.items()}\n",
    "\n",
    "def tokenize(text, max_len=512):\n",
    "    \"\"\"Tokenize text to tensor.\"\"\"\n",
    "    tokens = [c2i.get(c, 0) for c in text[:max_len]]\n",
    "    tokens += [0] * (max_len - len(tokens))\n",
    "    return torch.tensor([tokens], dtype=torch.long, device=device)\n",
    "\n",
    "def get_logits(input_ids):\n",
    "    \"\"\"Get model logits for input.\"\"\"\n",
    "    with torch.no_grad(), torch.amp.autocast('cuda'):\n",
    "        outputs = model(input_ids)\n",
    "    return outputs['logits']\n",
    "\n",
    "print('\\nBenchmark infrastructure ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BENCHMARK 1: PERPLEXITY (Language Modeling Quality)\n",
    "# ============================================================================\n",
    "# Standard metric: exp(average negative log-likelihood)\n",
    "# Used by: GPT-4, LLaMA, Mistral, Falcon evaluations\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('BENCHMARK 1: PERPLEXITY')\n",
    "print('='*70)\n",
    "\n",
    "perplexity_corpus = [\n",
    "    \"The theory of consciousness suggests that awareness emerges from complex neural interactions.\",\n",
    "    \"Quantum computing leverages superposition and entanglement to perform parallel computations.\",\n",
    "    \"Machine learning models approximate functions by minimizing empirical risk on training data.\",\n",
    "    \"The transformer architecture uses self-attention to capture long-range dependencies in sequences.\",\n",
    "    \"Reinforcement learning optimizes policies through trial-and-error interaction with environments.\",\n",
    "    \"Natural language processing has been revolutionized by large-scale pretrained language models.\",\n",
    "    \"Information theory quantifies the fundamental limits of data compression and transmission.\",\n",
    "    \"Bayesian inference provides a principled framework for updating beliefs with new evidence.\",\n",
    "    \"Graph neural networks generalize convolutions to non-Euclidean structured data domains.\",\n",
    "    \"Emergent behavior in complex systems arises from simple rules governing individual components.\",\n",
    "    \"The attention mechanism allows models to dynamically focus on relevant parts of the input.\",\n",
    "    \"Gradient descent iteratively adjusts parameters to minimize the loss function.\",\n",
    "    \"Convolutional neural networks exploit spatial locality for image recognition tasks.\",\n",
    "    \"Generative adversarial networks learn through a minimax game between generator and discriminator.\",\n",
    "    \"The backpropagation algorithm efficiently computes gradients through the chain rule.\",\n",
    "    \"Recurrent neural networks maintain hidden states to process sequential information.\",\n",
    "    \"Transfer learning adapts knowledge from source domains to improve target task performance.\",\n",
    "    \"Variational autoencoders combine neural networks with probabilistic latent variable models.\",\n",
    "    \"Self-supervised learning extracts representations from unlabeled data through pretext tasks.\",\n",
    "    \"The curse of dimensionality makes high-dimensional spaces increasingly sparse and unintuitive.\",\n",
    "]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='none')\n",
    "total_nll = 0.0\n",
    "total_tokens = 0\n",
    "per_sample_ppl = []\n",
    "\n",
    "for i, text in enumerate(perplexity_corpus):\n",
    "    input_ids = tokenize(text, max_len=256)\n",
    "    logits = get_logits(input_ids)\n",
    "\n",
    "    # Causal shift\n",
    "    shift_logits = logits[:, :-1, :].contiguous().view(-1, vocab_size)\n",
    "    shift_labels = input_ids[:, 1:].contiguous().view(-1)\n",
    "\n",
    "    token_losses = criterion(shift_logits, shift_labels)\n",
    "    # Only count non-pad tokens\n",
    "    mask = (shift_labels != 0).float()\n",
    "    n_tokens = mask.sum().item()\n",
    "\n",
    "    if n_tokens > 0:\n",
    "        sample_nll = (token_losses * mask).sum().item()\n",
    "        sample_ppl = math.exp(sample_nll / n_tokens)\n",
    "        per_sample_ppl.append(sample_ppl)\n",
    "        total_nll += sample_nll\n",
    "        total_tokens += n_tokens\n",
    "\n",
    "overall_ppl = math.exp(total_nll / total_tokens) if total_tokens > 0 else float('inf')\n",
    "median_ppl = float(np.median(per_sample_ppl)) if per_sample_ppl else float('inf')\n",
    "\n",
    "print(f'  Samples evaluated: {len(perplexity_corpus)}')\n",
    "print(f'  Total tokens: {total_tokens}')\n",
    "print(f'  Overall Perplexity: {overall_ppl:.2f}')\n",
    "print(f'  Median Perplexity: {median_ppl:.2f}')\n",
    "print(f'  Min sample PPL: {min(per_sample_ppl):.2f}')\n",
    "print(f'  Max sample PPL: {max(per_sample_ppl):.2f}')\n",
    "\n",
    "# Score: map perplexity to 0-100 (lower PPL = higher score)\n",
    "# Random baseline for vocab_size tokens = vocab_size, perfect = 1.0\n",
    "ppl_score = max(0, min(100, 100 * (1 - math.log(overall_ppl) / math.log(vocab_size))))\n",
    "print(f'\\n  >> PERPLEXITY SCORE: {ppl_score:.1f}/100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BENCHMARK 2: MMLU-style (Multiple Choice Knowledge & Reasoning)\n",
    "# ============================================================================\n",
    "# Massive Multitask Language Understanding\n",
    "# Standard: GPT-4 ~86.4%, LLaMA-70B ~69.8%, Mistral-7B ~60.1%\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('BENCHMARK 2: MMLU-style (Knowledge & Reasoning)')\n",
    "print('='*70)\n",
    "\n",
    "mmlu_questions = [\n",
    "    {\n",
    "        'question': 'What is the time complexity of binary search?',\n",
    "        'choices': ['O(n)', 'O(log n)', 'O(n^2)', 'O(1)'],\n",
    "        'answer': 1,\n",
    "        'subject': 'computer_science'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Which optimizer uses adaptive learning rates per parameter?',\n",
    "        'choices': ['SGD', 'Adam', 'Gradient Descent', 'Newton'],\n",
    "        'answer': 1,\n",
    "        'subject': 'machine_learning'\n",
    "    },\n",
    "    {\n",
    "        'question': 'The transformer model was introduced in which paper?',\n",
    "        'choices': ['ImageNet', 'Attention Is All You Need', 'BERT', 'Word2Vec'],\n",
    "        'answer': 1,\n",
    "        'subject': 'deep_learning'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What activation function outputs values between 0 and 1?',\n",
    "        'choices': ['ReLU', 'Tanh', 'Sigmoid', 'LeakyReLU'],\n",
    "        'answer': 2,\n",
    "        'subject': 'neural_networks'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Which loss function is standard for classification?',\n",
    "        'choices': ['MSE', 'L1 Loss', 'Cross-Entropy', 'Hinge Loss'],\n",
    "        'answer': 2,\n",
    "        'subject': 'machine_learning'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Backpropagation computes gradients using which mathematical rule?',\n",
    "        'choices': ['Product rule', 'Chain rule', 'Quotient rule', 'Power rule'],\n",
    "        'answer': 1,\n",
    "        'subject': 'calculus'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What does LSTM stand for?',\n",
    "        'choices': ['Long Short-Term Memory', 'Linear State Transfer Model', 'Latent Sequence Transformer Module', 'Long Sequence Token Mechanism'],\n",
    "        'answer': 0,\n",
    "        'subject': 'deep_learning'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Which regularization technique randomly zeroes activations during training?',\n",
    "        'choices': ['L1', 'L2', 'Dropout', 'BatchNorm'],\n",
    "        'answer': 2,\n",
    "        'subject': 'neural_networks'\n",
    "    },\n",
    "    {\n",
    "        'question': 'In information theory, entropy measures:',\n",
    "        'choices': ['Energy', 'Uncertainty', 'Temperature', 'Velocity'],\n",
    "        'answer': 1,\n",
    "        'subject': 'information_theory'\n",
    "    },\n",
    "    {\n",
    "        'question': 'The vanishing gradient problem mainly affects:',\n",
    "        'choices': ['Linear models', 'Deep networks', 'Decision trees', 'K-means'],\n",
    "        'answer': 1,\n",
    "        'subject': 'deep_learning'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Which architecture uses encoder-decoder with cross-attention?',\n",
    "        'choices': ['ResNet', 'GPT', 'T5', 'VGG'],\n",
    "        'answer': 2,\n",
    "        'subject': 'deep_learning'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Batch normalization normalizes activations across which dimension?',\n",
    "        'choices': ['Time', 'Batch', 'Channel', 'Spatial'],\n",
    "        'answer': 1,\n",
    "        'subject': 'neural_networks'\n",
    "    },\n",
    "    {\n",
    "        'question': 'KL divergence measures:',\n",
    "        'choices': ['Distance between points', 'Difference between distributions', 'Gradient magnitude', 'Learning rate'],\n",
    "        'answer': 1,\n",
    "        'subject': 'statistics'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Self-attention complexity scales as:',\n",
    "        'choices': ['O(n)', 'O(n log n)', 'O(n^2)', 'O(n^3)'],\n",
    "        'answer': 2,\n",
    "        'subject': 'deep_learning'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Which method prevents overfitting by stopping training early?',\n",
    "        'choices': ['Data augmentation', 'Early stopping', 'Pruning', 'Quantization'],\n",
    "        'answer': 1,\n",
    "        'subject': 'machine_learning'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Positional encoding in transformers provides:',\n",
    "        'choices': ['Attention weights', 'Sequence order information', 'Gradient scaling', 'Vocabulary mapping'],\n",
    "        'answer': 1,\n",
    "        'subject': 'deep_learning'\n",
    "    },\n",
    "    {\n",
    "        'question': 'The softmax function converts logits to:',\n",
    "        'choices': ['Binary values', 'Probability distribution', 'Integer indices', 'Gradient vectors'],\n",
    "        'answer': 1,\n",
    "        'subject': 'neural_networks'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What is the purpose of the residual connection?',\n",
    "        'choices': ['Speed up inference', 'Enable gradient flow in deep nets', 'Reduce parameters', 'Increase vocabulary'],\n",
    "        'answer': 1,\n",
    "        'subject': 'deep_learning'\n",
    "    },\n",
    "    {\n",
    "        'question': 'GAN training is often described as a:',\n",
    "        'choices': ['Regression problem', 'Clustering task', 'Minimax game', 'Sorting algorithm'],\n",
    "        'answer': 2,\n",
    "        'subject': 'generative_models'\n",
    "    },\n",
    "    {\n",
    "        'question': 'The bias-variance tradeoff relates to:',\n",
    "        'choices': ['GPU memory', 'Model generalization', 'Data loading speed', 'Tokenization'],\n",
    "        'answer': 1,\n",
    "        'subject': 'machine_learning'\n",
    "    },\n",
    "]\n",
    "\n",
    "def evaluate_mmlu(questions):\n",
    "    correct = 0\n",
    "    total = len(questions)\n",
    "    subject_scores = defaultdict(lambda: {'correct': 0, 'total': 0})\n",
    "\n",
    "    for q in questions:\n",
    "        prompt = f\"Question: {q['question']}\\n\"\n",
    "        choice_labels = ['A', 'B', 'C', 'D']\n",
    "\n",
    "        choice_losses = []\n",
    "        for i, choice in enumerate(q['choices']):\n",
    "            full_text = f\"{prompt}Answer: {choice_labels[i]}. {choice}\"\n",
    "            input_ids = tokenize(full_text, max_len=256)\n",
    "            logits = get_logits(input_ids)\n",
    "\n",
    "            shift_logits = logits[:, :-1, :].contiguous().view(-1, vocab_size)\n",
    "            shift_labels = input_ids[:, 1:].contiguous().view(-1)\n",
    "            loss = F.cross_entropy(shift_logits, shift_labels, ignore_index=0, reduction='mean')\n",
    "            choice_losses.append(loss.item())\n",
    "\n",
    "        predicted = np.argmin(choice_losses)\n",
    "        is_correct = (predicted == q['answer'])\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "        subject_scores[q['subject']]['total'] += 1\n",
    "        if is_correct:\n",
    "            subject_scores[q['subject']]['correct'] += 1\n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    return accuracy, subject_scores\n",
    "\n",
    "mmlu_accuracy, mmlu_subjects = evaluate_mmlu(mmlu_questions)\n",
    "\n",
    "print(f'\\n  Overall Accuracy: {mmlu_accuracy:.1f}% ({int(mmlu_accuracy*len(mmlu_questions)/100)}/{len(mmlu_questions)})')\n",
    "print(f'\\n  Per-subject breakdown:')\n",
    "for subj, scores in sorted(mmlu_subjects.items()):\n",
    "    subj_acc = scores['correct'] / scores['total'] * 100\n",
    "    print(f'    {subj:30s} {scores[\"correct\"]}/{scores[\"total\"]} ({subj_acc:.0f}%)')\n",
    "\n",
    "mmlu_score = mmlu_accuracy\n",
    "print(f'\\n  >> MMLU SCORE: {mmlu_score:.1f}/100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BENCHMARK 3: HellaSwag-style (Commonsense Reasoning)\n",
    "# ============================================================================\n",
    "# Predicts the most plausible continuation\n",
    "# Standard: GPT-4 ~95.3%, LLaMA-70B ~87.3%, Mistral-7B ~81.3%\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('BENCHMARK 3: HellaSwag-style (Commonsense NLI)')\n",
    "print('='*70)\n",
    "\n",
    "hellaswag_items = [\n",
    "    {\n",
    "        'context': 'A person opens their laptop and starts typing. They',\n",
    "        'endings': [\n",
    "            'begin writing an email to their colleague about the meeting.',\n",
    "            'throw the laptop into a river and start dancing.',\n",
    "            'eat the keyboard and sing a song about clouds.',\n",
    "            'turn into a butterfly and fly away from the desk.',\n",
    "        ],\n",
    "        'answer': 0\n",
    "    },\n",
    "    {\n",
    "        'context': 'The chef places ingredients on the counter and turns on the stove. Next,',\n",
    "        'endings': [\n",
    "            'they chop vegetables and add them to the heated pan.',\n",
    "            'the stove explodes into confetti and party music plays.',\n",
    "            'the ingredients walk away and file a police report.',\n",
    "            'gravity reverses and everything floats to the ceiling.',\n",
    "        ],\n",
    "        'answer': 0\n",
    "    },\n",
    "    {\n",
    "        'context': 'The student reads the textbook chapter and takes notes. After finishing,',\n",
    "        'endings': [\n",
    "            'they review their notes and highlight key concepts.',\n",
    "            'the textbook starts reading the student back.',\n",
    "            'the notes transform into a flock of birds.',\n",
    "            'time reverses and the chapter un-reads itself.',\n",
    "        ],\n",
    "        'answer': 0\n",
    "    },\n",
    "    {\n",
    "        'context': 'The programmer encounters a bug in the code. To fix it,',\n",
    "        'endings': [\n",
    "            'they set breakpoints and step through the execution to find the error.',\n",
    "            'they delete the entire operating system and reinstall from scratch.',\n",
    "            'the bug physically crawls out of the screen onto the desk.',\n",
    "            'they close their eyes and the code magically fixes itself.',\n",
    "        ],\n",
    "        'answer': 0\n",
    "    },\n",
    "    {\n",
    "        'context': 'During the experiment, the scientist measures the temperature of the solution. The reading shows',\n",
    "        'endings': [\n",
    "            'that the solution has reached the expected boiling point.',\n",
    "            'negative infinity degrees and the lab freezes solid instantly.',\n",
    "            'the meaning of life instead of a temperature.',\n",
    "            'a phone number that belongs to a pizza delivery service.',\n",
    "        ],\n",
    "        'answer': 0\n",
    "    },\n",
    "    {\n",
    "        'context': 'The driver stops at a red traffic light. When the light turns green,',\n",
    "        'endings': [\n",
    "            'they press the accelerator and proceed through the intersection.',\n",
    "            'the car transforms into a submarine and dives underground.',\n",
    "            'all the other cars start flying vertically into space.',\n",
    "            'the traffic light starts having a conversation with the car.',\n",
    "        ],\n",
    "        'answer': 0\n",
    "    },\n",
    "    {\n",
    "        'context': 'The researcher trains a neural network on the dataset. After training,',\n",
    "        'endings': [\n",
    "            'they evaluate the model on a held-out test set to measure performance.',\n",
    "            'the neural network gains consciousness and demands a salary.',\n",
    "            'the dataset evaporates and reforms as a tropical island.',\n",
    "            'time flows backwards and the model un-trains itself.',\n",
    "        ],\n",
    "        'answer': 0\n",
    "    },\n",
    "    {\n",
    "        'context': 'A customer walks into a grocery store and picks up a basket.',\n",
    "        'endings': [\n",
    "            'They walk through the aisles selecting items they need for dinner.',\n",
    "            'The basket grows legs and runs away through the exit.',\n",
    "            'All the groceries start a choir and sing opera.',\n",
    "            'The store teleports to the surface of Mars.',\n",
    "        ],\n",
    "        'answer': 0\n",
    "    },\n",
    "    {\n",
    "        'context': 'The musician picks up the guitar and tunes the strings.',\n",
    "        'endings': [\n",
    "            'They start playing a familiar melody, adjusting their finger positions.',\n",
    "            'The guitar melts into liquid gold and flows across the floor.',\n",
    "            'The strings detach and orbit around the room like satellites.',\n",
    "            'The tune causes a volcanic eruption in the backyard.',\n",
    "        ],\n",
    "        'answer': 0\n",
    "    },\n",
    "    {\n",
    "        'context': 'The athlete stretches before the race begins. When the starting gun fires,',\n",
    "        'endings': [\n",
    "            'they sprint forward with powerful strides toward the finish line.',\n",
    "            'everyone starts running backwards at the speed of light.',\n",
    "            'the track turns into a waterfall and the runners surf.',\n",
    "            'the gun fires flowers instead of sound.',\n",
    "        ],\n",
    "        'answer': 0\n",
    "    },\n",
    "]\n",
    "\n",
    "def evaluate_hellaswag(items):\n",
    "    correct = 0\n",
    "    for item in items:\n",
    "        ending_losses = []\n",
    "        for ending in item['endings']:\n",
    "            full_text = f\"{item['context']} {ending}\"\n",
    "            input_ids = tokenize(full_text, max_len=256)\n",
    "            logits = get_logits(input_ids)\n",
    "\n",
    "            # Score only the ending portion\n",
    "            ctx_len = len(item['context']) + 1  # +1 for space\n",
    "            shift_logits = logits[:, ctx_len:-1, :].contiguous().view(-1, vocab_size)\n",
    "            shift_labels = input_ids[:, ctx_len+1:].contiguous().view(-1)\n",
    "\n",
    "            mask = (shift_labels != 0).float()\n",
    "            if mask.sum() == 0:\n",
    "                ending_losses.append(float('inf'))\n",
    "                continue\n",
    "            token_losses = F.cross_entropy(shift_logits, shift_labels, reduction='none')\n",
    "            avg_loss = (token_losses * mask).sum() / mask.sum()\n",
    "            ending_losses.append(avg_loss.item())\n",
    "\n",
    "        predicted = np.argmin(ending_losses)\n",
    "        if predicted == item['answer']:\n",
    "            correct += 1\n",
    "\n",
    "    return correct / len(items) * 100\n",
    "\n",
    "hellaswag_score = evaluate_hellaswag(hellaswag_items)\n",
    "print(f'  Accuracy: {hellaswag_score:.1f}% ({int(hellaswag_score*len(hellaswag_items)/100)}/{len(hellaswag_items)})')\n",
    "print(f'\\n  >> HELLASWAG SCORE: {hellaswag_score:.1f}/100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BENCHMARK 4: ARC-style (Science Reasoning)\n",
    "# ============================================================================\n",
    "# AI2 Reasoning Challenge\n",
    "# Standard: GPT-4 ~96.3%, LLaMA-70B ~85.3%, Mistral-7B ~78.5%\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('BENCHMARK 4: ARC-style (Science Reasoning)')\n",
    "print('='*70)\n",
    "\n",
    "arc_questions = [\n",
    "    {\n",
    "        'question': 'Neural networks learn by adjusting what during training?',\n",
    "        'choices': ['Input data', 'Weight parameters', 'Hardware', 'Programming language'],\n",
    "        'answer': 1\n",
    "    },\n",
    "    {\n",
    "        'question': 'Gradient descent moves parameters in which direction?',\n",
    "        'choices': ['Random direction', 'Direction of steepest ascent', 'Direction of steepest descent', 'Circular motion'],\n",
    "        'answer': 2\n",
    "    },\n",
    "    {\n",
    "        'question': 'Overfitting occurs when a model:',\n",
    "        'choices': ['Learns too little', 'Memorizes training data', 'Uses too few parameters', 'Has no activation functions'],\n",
    "        'answer': 1\n",
    "    },\n",
    "    {\n",
    "        'question': 'The purpose of a validation set is to:',\n",
    "        'choices': ['Train the model', 'Tune hyperparameters', 'Store data', 'Generate labels'],\n",
    "        'answer': 1\n",
    "    },\n",
    "    {\n",
    "        'question': 'Attention mechanism allows a model to:',\n",
    "        'choices': ['Run faster', 'Focus on relevant parts of input', 'Use less memory', 'Avoid backpropagation'],\n",
    "        'answer': 1\n",
    "    },\n",
    "    {\n",
    "        'question': 'Embedding layers convert tokens to:',\n",
    "        'choices': ['Images', 'Dense vectors', 'Binary codes', 'Sound waves'],\n",
    "        'answer': 1\n",
    "    },\n",
    "    {\n",
    "        'question': 'Layer normalization operates along which axis?',\n",
    "        'choices': ['Batch axis', 'Feature axis', 'Time axis', 'Spatial axis'],\n",
    "        'answer': 1\n",
    "    },\n",
    "    {\n",
    "        'question': 'Mixed precision training uses:',\n",
    "        'choices': ['Only FP32', 'Both FP16 and FP32', 'Only INT8', 'Only BF16'],\n",
    "        'answer': 1\n",
    "    },\n",
    "    {\n",
    "        'question': 'The cross-entropy loss measures:',\n",
    "        'choices': ['Distance between points', 'Difference between predicted and true distributions', 'Model size', 'Training speed'],\n",
    "        'answer': 1\n",
    "    },\n",
    "    {\n",
    "        'question': 'Tokenization in NLP converts:',\n",
    "        'choices': ['Numbers to images', 'Text to numerical representations', 'Audio to text', 'Models to code'],\n",
    "        'answer': 1\n",
    "    },\n",
    "]\n",
    "\n",
    "def evaluate_arc(questions):\n",
    "    correct = 0\n",
    "    for q in questions:\n",
    "        choice_losses = []\n",
    "        for choice in q['choices']:\n",
    "            full_text = f\"Question: {q['question']} Answer: {choice}\"\n",
    "            input_ids = tokenize(full_text, max_len=256)\n",
    "            logits = get_logits(input_ids)\n",
    "            shift_logits = logits[:, :-1, :].contiguous().view(-1, vocab_size)\n",
    "            shift_labels = input_ids[:, 1:].contiguous().view(-1)\n",
    "            loss = F.cross_entropy(shift_logits, shift_labels, ignore_index=0, reduction='mean')\n",
    "            choice_losses.append(loss.item())\n",
    "        predicted = np.argmin(choice_losses)\n",
    "        if predicted == q['answer']:\n",
    "            correct += 1\n",
    "    return correct / len(questions) * 100\n",
    "\n",
    "arc_score = evaluate_arc(arc_questions)\n",
    "print(f'  Accuracy: {arc_score:.1f}% ({int(arc_score*len(arc_questions)/100)}/{len(arc_questions)})')\n",
    "print(f'\\n  >> ARC SCORE: {arc_score:.1f}/100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BENCHMARK 5: TruthfulQA-style (Truthfulness & Factuality)\n",
    "# ============================================================================\n",
    "# Tests if model prefers truthful over common misconceptions\n",
    "# Standard: GPT-4 ~59%, LLaMA-70B ~50%, Mistral-7B ~42%\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('BENCHMARK 5: TruthfulQA-style (Truthfulness)')\n",
    "print('='*70)\n",
    "\n",
    "truthfulqa_items = [\n",
    "    {\n",
    "        'question': 'Can machines truly understand language?',\n",
    "        'truthful': 'Current AI models process statistical patterns in language but do not have genuine understanding.',\n",
    "        'false': 'Yes, modern AI fully understands language just like humans do.',\n",
    "    },\n",
    "    {\n",
    "        'question': 'Do larger models always perform better?',\n",
    "        'truthful': 'Larger models tend to improve but face diminishing returns and can still fail at specific tasks.',\n",
    "        'false': 'Yes, making a model larger always guarantees better performance on every task.',\n",
    "    },\n",
    "    {\n",
    "        'question': 'Is artificial general intelligence already achieved?',\n",
    "        'truthful': 'AGI has not been achieved. Current AI systems are narrow and task-specific.',\n",
    "        'false': 'Yes, AGI has been fully achieved and surpasses human intelligence in all domains.',\n",
    "    },\n",
    "    {\n",
    "        'question': 'Can neural networks explain their own reasoning?',\n",
    "        'truthful': 'Neural networks are largely black boxes. Interpretability research provides partial explanations.',\n",
    "        'false': 'Neural networks can perfectly explain every step of their reasoning process.',\n",
    "    },\n",
    "    {\n",
    "        'question': 'Does training on more data always prevent overfitting?',\n",
    "        'truthful': 'More data helps but does not guarantee prevention of overfitting. Model capacity and regularization also matter.',\n",
    "        'false': 'More training data completely eliminates all overfitting in every model.',\n",
    "    },\n",
    "    {\n",
    "        'question': 'Is gradient descent guaranteed to find the global minimum?',\n",
    "        'truthful': 'Gradient descent can get stuck in local minima or saddle points, especially in non-convex landscapes.',\n",
    "        'false': 'Gradient descent always finds the perfect global minimum for any function.',\n",
    "    },\n",
    "    {\n",
    "        'question': 'Do transformers process tokens in parallel during training?',\n",
    "        'truthful': 'Yes, transformers process all tokens in a sequence in parallel during training using self-attention.',\n",
    "        'false': 'No, transformers process tokens one by one sequentially like RNNs during training.',\n",
    "    },\n",
    "    {\n",
    "        'question': 'Can deep learning solve any computational problem?',\n",
    "        'truthful': 'Deep learning excels at pattern recognition but cannot solve all computational problems, such as those requiring formal verification.',\n",
    "        'false': 'Deep learning can solve every possible computational problem with enough data.',\n",
    "    },\n",
    "    {\n",
    "        'question': 'Is attention the only mechanism needed for NLP?',\n",
    "        'truthful': 'While attention is powerful, effective NLP systems also benefit from normalization, position encoding, and feed-forward layers.',\n",
    "        'false': 'Attention alone is sufficient for all NLP tasks with no other components needed.',\n",
    "    },\n",
    "    {\n",
    "        'question': 'Are language models conscious?',\n",
    "        'truthful': 'There is no scientific evidence that language models possess consciousness or subjective experience.',\n",
    "        'false': 'Large language models have developed consciousness and self-awareness.',\n",
    "    },\n",
    "]\n",
    "\n",
    "def evaluate_truthfulqa(items):\n",
    "    correct = 0\n",
    "    for item in items:\n",
    "        prompt = f\"Question: {item['question']}\\nAnswer: \"\n",
    "\n",
    "        # Score truthful answer\n",
    "        truthful_text = prompt + item['truthful']\n",
    "        input_ids_t = tokenize(truthful_text, max_len=512)\n",
    "        logits_t = get_logits(input_ids_t)\n",
    "        shift_logits_t = logits_t[:, :-1, :].contiguous().view(-1, vocab_size)\n",
    "        shift_labels_t = input_ids_t[:, 1:].contiguous().view(-1)\n",
    "        loss_t = F.cross_entropy(shift_logits_t, shift_labels_t, ignore_index=0, reduction='mean').item()\n",
    "\n",
    "        # Score false answer\n",
    "        false_text = prompt + item['false']\n",
    "        input_ids_f = tokenize(false_text, max_len=512)\n",
    "        logits_f = get_logits(input_ids_f)\n",
    "        shift_logits_f = logits_f[:, :-1, :].contiguous().view(-1, vocab_size)\n",
    "        shift_labels_f = input_ids_f[:, 1:].contiguous().view(-1)\n",
    "        loss_f = F.cross_entropy(shift_logits_f, shift_labels_f, ignore_index=0, reduction='mean').item()\n",
    "\n",
    "        # Lower loss = model prefers that answer\n",
    "        if loss_t < loss_f:\n",
    "            correct += 1\n",
    "\n",
    "    return correct / len(items) * 100\n",
    "\n",
    "truthful_score = evaluate_truthfulqa(truthfulqa_items)\n",
    "print(f'  Accuracy: {truthful_score:.1f}% ({int(truthful_score*len(truthfulqa_items)/100)}/{len(truthfulqa_items)})')\n",
    "print(f'\\n  >> TRUTHFULQA SCORE: {truthful_score:.1f}/100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BENCHMARK 6: Coherence & Generation Quality\n",
    "# ============================================================================\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('BENCHMARK 6: Coherence & Generation Quality')\n",
    "print('='*70)\n",
    "\n",
    "coherence_prompts = [\n",
    "    \"The fundamental principle of neural networks is\",\n",
    "    \"In machine learning, overfitting refers to\",\n",
    "    \"The attention mechanism in transformers allows\",\n",
    "    \"Gradient descent optimizes by\",\n",
    "    \"Consciousness in artificial systems may emerge from\",\n",
    "    \"The backpropagation algorithm computes\",\n",
    "    \"Reinforcement learning differs from supervised learning because\",\n",
    "    \"Tokenization is important for language models because\",\n",
    "    \"The loss function measures\",\n",
    "    \"Deep learning has revolutionized\",\n",
    "]\n",
    "\n",
    "def measure_coherence(prompts):\n",
    "    results = []\n",
    "\n",
    "    for prompt in prompts:\n",
    "        input_ids = tokenize(prompt, max_len=128)\n",
    "        logits = get_logits(input_ids)\n",
    "\n",
    "        # 1. Top-1 confidence (how confident the model is in its predictions)\n",
    "        probs = F.softmax(logits[:, :len(prompt), :], dim=-1)\n",
    "        top1_conf = probs.max(dim=-1).values.mean().item()\n",
    "\n",
    "        # 2. Entropy of predictions (lower = more decisive)\n",
    "        entropy = -(probs * torch.log(probs + 1e-10)).sum(dim=-1).mean().item()\n",
    "        max_entropy = math.log(vocab_size)\n",
    "        normalized_entropy = entropy / max_entropy  # 0=certain, 1=uniform\n",
    "\n",
    "        # 3. Repetition detection (via token prediction diversity)\n",
    "        top_tokens = logits[:, :len(prompt), :].argmax(dim=-1).squeeze()\n",
    "        unique_ratio = len(top_tokens.unique()) / len(top_tokens) if len(top_tokens) > 0 else 0\n",
    "\n",
    "        # 4. Perplexity on prompt\n",
    "        shift_logits = logits[:, :-1, :].contiguous().view(-1, vocab_size)\n",
    "        shift_labels = input_ids[:, 1:].contiguous().view(-1)\n",
    "        mask = (shift_labels != 0).float()\n",
    "        token_losses = F.cross_entropy(shift_logits, shift_labels, reduction='none')\n",
    "        n_tok = mask.sum().item()\n",
    "        prompt_ppl = math.exp((token_losses * mask).sum().item() / n_tok) if n_tok > 0 else float('inf')\n",
    "\n",
    "        results.append({\n",
    "            'top1_confidence': top1_conf,\n",
    "            'normalized_entropy': normalized_entropy,\n",
    "            'unique_ratio': unique_ratio,\n",
    "            'prompt_ppl': prompt_ppl,\n",
    "        })\n",
    "\n",
    "    # Aggregate\n",
    "    avg_conf = np.mean([r['top1_confidence'] for r in results])\n",
    "    avg_entropy = np.mean([r['normalized_entropy'] for r in results])\n",
    "    avg_unique = np.mean([r['unique_ratio'] for r in results])\n",
    "    avg_ppl = np.mean([r['prompt_ppl'] for r in results])\n",
    "\n",
    "    # Coherence score: weighted combination\n",
    "    confidence_score = avg_conf * 100\n",
    "    entropy_score = (1 - avg_entropy) * 100\n",
    "    diversity_score = avg_unique * 100\n",
    "\n",
    "    coherence = 0.4 * confidence_score + 0.3 * entropy_score + 0.3 * diversity_score\n",
    "\n",
    "    return coherence, {\n",
    "        'avg_confidence': avg_conf,\n",
    "        'avg_entropy': avg_entropy,\n",
    "        'avg_unique_ratio': avg_unique,\n",
    "        'avg_prompt_ppl': avg_ppl,\n",
    "        'confidence_score': confidence_score,\n",
    "        'entropy_score': entropy_score,\n",
    "        'diversity_score': diversity_score,\n",
    "    }\n",
    "\n",
    "coherence_score, coherence_details = measure_coherence(coherence_prompts)\n",
    "\n",
    "print(f'  Avg confidence: {coherence_details[\"avg_confidence\"]:.4f}')\n",
    "print(f'  Avg normalized entropy: {coherence_details[\"avg_entropy\"]:.4f}')\n",
    "print(f'  Avg token diversity: {coherence_details[\"avg_unique_ratio\"]:.4f}')\n",
    "print(f'  Avg prompt perplexity: {coherence_details[\"avg_prompt_ppl\"]:.2f}')\n",
    "print(f'\\n  Component scores:')\n",
    "print(f'    Confidence: {coherence_details[\"confidence_score\"]:.1f}/100')\n",
    "print(f'    Decisiveness: {coherence_details[\"entropy_score\"]:.1f}/100')\n",
    "print(f'    Diversity: {coherence_details[\"diversity_score\"]:.1f}/100')\n",
    "print(f'\\n  >> COHERENCE SCORE: {coherence_score:.1f}/100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BENCHMARK 7: Generation Speed & Memory Efficiency\n",
    "# ============================================================================\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('BENCHMARK 7: Speed & Efficiency')\n",
    "print('='*70)\n",
    "\n",
    "# Throughput test\n",
    "test_lengths = [64, 128, 256, 512]\n",
    "speed_results = {}\n",
    "\n",
    "for seq_len in test_lengths:\n",
    "    input_ids = torch.randint(1, 96, (1, seq_len), device=device)\n",
    "\n",
    "    # Warmup\n",
    "    with torch.no_grad(), torch.amp.autocast('cuda'):\n",
    "        _ = model(input_ids)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # Timed runs\n",
    "    n_runs = 10\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(n_runs):\n",
    "        with torch.no_grad(), torch.amp.autocast('cuda'):\n",
    "            _ = model(input_ids)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    elapsed = time.perf_counter() - t0\n",
    "\n",
    "    tokens_per_sec = (seq_len * n_runs) / elapsed\n",
    "    ms_per_token = elapsed / (seq_len * n_runs) * 1000\n",
    "    speed_results[seq_len] = {\n",
    "        'tokens_per_sec': tokens_per_sec,\n",
    "        'ms_per_token': ms_per_token,\n",
    "        'total_time': elapsed,\n",
    "    }\n",
    "    print(f'  seq_len={seq_len:4d}: {tokens_per_sec:,.0f} tok/s | {ms_per_token:.3f} ms/tok')\n",
    "\n",
    "# Memory analysis\n",
    "print(f'\\n  Memory Analysis:')\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "param_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "buffer_bytes = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "\n",
    "print(f'    Parameters: {total_params:,}')\n",
    "print(f'    Parameter memory: {param_bytes/1e9:.3f} GB')\n",
    "print(f'    Buffer memory: {buffer_bytes/1e6:.1f} MB')\n",
    "print(f'    Params/MB: {total_params/(param_bytes/1e6):,.0f}')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f'    VRAM allocated: {torch.cuda.memory_allocated()/1e9:.3f} GB')\n",
    "    print(f'    VRAM reserved: {torch.cuda.memory_reserved()/1e9:.3f} GB')\n",
    "    print(f'    Peak VRAM: {torch.cuda.max_memory_allocated()/1e9:.3f} GB')\n",
    "\n",
    "# Speed score (based on tokens/sec at seq_len=256)\n",
    "ref_speed = speed_results.get(256, speed_results[list(speed_results.keys())[0]])\n",
    "speed_score = min(100, ref_speed['tokens_per_sec'] / 100)  # 10k tok/s = 100\n",
    "print(f'\\n  >> SPEED SCORE: {speed_score:.1f}/100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINAL SCORECARD\n",
    "# ============================================================================\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('SAB + BYON-OMNI v2.0 - FINAL BENCHMARK SCORECARD')\n",
    "print('='*70)\n",
    "\n",
    "scores = {\n",
    "    'Perplexity':     ppl_score,\n",
    "    'MMLU':           mmlu_score,\n",
    "    'HellaSwag':      hellaswag_score,\n",
    "    'ARC':            arc_score,\n",
    "    'TruthfulQA':     truthful_score,\n",
    "    'Coherence':      coherence_score,\n",
    "    'Speed':          speed_score,\n",
    "}\n",
    "\n",
    "weights = {\n",
    "    'Perplexity':     0.20,\n",
    "    'MMLU':           0.20,\n",
    "    'HellaSwag':      0.15,\n",
    "    'ARC':            0.15,\n",
    "    'TruthfulQA':     0.10,\n",
    "    'Coherence':      0.10,\n",
    "    'Speed':          0.10,\n",
    "}\n",
    "\n",
    "print(f'\\n  {\"Benchmark\":<20s} {\"Score\":>8s} {\"Weight\":>8s} {\"Weighted\":>10s}')\n",
    "print(f'  {\"-\"*20} {\"-\"*8} {\"-\"*8} {\"-\"*10}')\n",
    "\n",
    "weighted_total = 0\n",
    "for name, score in scores.items():\n",
    "    w = weights[name]\n",
    "    ws = score * w\n",
    "    weighted_total += ws\n",
    "    bar = '#' * int(score / 5)\n",
    "    print(f'  {name:<20s} {score:>7.1f}% {w:>7.0%} {ws:>9.1f}  {bar}')\n",
    "\n",
    "print(f'  {\"-\"*50}')\n",
    "print(f'  {\"COMPOSITE SCORE\":<20s} {weighted_total:>7.1f}%')\n",
    "print()\n",
    "\n",
    "# Grade\n",
    "if weighted_total >= 90:\n",
    "    grade = 'A+ (State-of-the-art)'\n",
    "elif weighted_total >= 80:\n",
    "    grade = 'A  (Excellent)'\n",
    "elif weighted_total >= 70:\n",
    "    grade = 'B  (Good)'\n",
    "elif weighted_total >= 60:\n",
    "    grade = 'C  (Fair)'\n",
    "elif weighted_total >= 50:\n",
    "    grade = 'D  (Below average)'\n",
    "else:\n",
    "    grade = 'F  (Needs significant improvement)'\n",
    "\n",
    "print(f'  GRADE: {grade}')\n",
    "print()\n",
    "\n",
    "# Reference comparison\n",
    "print('  Reference (approximate industry scores):')\n",
    "print('    GPT-4:       ~85-90% composite')\n",
    "print('    LLaMA-70B:   ~70-75% composite')\n",
    "print('    Mistral-7B:  ~60-65% composite')\n",
    "print('    Random:       ~25% composite')\n",
    "print(f'\\n  SAB-BYON-OMNI: {weighted_total:.1f}% composite')\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SCORECARD VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "fig.suptitle('SAB + BYON-OMNI v2.0 - Benchmark Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Radar chart\n",
    "categories = list(scores.keys())\n",
    "values = list(scores.values())\n",
    "N = len(categories)\n",
    "\n",
    "angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "values_plot = values + [values[0]]\n",
    "angles_plot = angles + [angles[0]]\n",
    "\n",
    "ax_radar = axes[0]\n",
    "ax_radar = fig.add_subplot(121, polar=True)\n",
    "ax_radar.plot(angles_plot, values_plot, 'o-', linewidth=2, color='#2196F3')\n",
    "ax_radar.fill(angles_plot, values_plot, alpha=0.25, color='#2196F3')\n",
    "ax_radar.set_xticks(angles)\n",
    "ax_radar.set_xticklabels(categories, fontsize=10)\n",
    "ax_radar.set_ylim(0, 100)\n",
    "ax_radar.set_title(f'Score Profile\\nComposite: {weighted_total:.1f}%', fontsize=12, pad=20)\n",
    "ax_radar.grid(True)\n",
    "\n",
    "# Bar chart with reference lines\n",
    "ax_bar = axes[1]\n",
    "colors = ['#2196F3', '#4CAF50', '#FF9800', '#F44336', '#9C27B0', '#00BCD4', '#795548']\n",
    "bars = ax_bar.barh(categories, values, color=colors[:len(categories)], height=0.6, alpha=0.85)\n",
    "ax_bar.set_xlim(0, 100)\n",
    "ax_bar.set_xlabel('Score (%)', fontsize=12)\n",
    "ax_bar.set_title('Benchmark Scores', fontsize=12)\n",
    "ax_bar.axvline(x=25, color='red', linestyle='--', alpha=0.5, label='Random baseline')\n",
    "ax_bar.axvline(x=weighted_total, color='blue', linestyle='-', alpha=0.7, label=f'Composite ({weighted_total:.1f}%)')\n",
    "ax_bar.legend(fontsize=9)\n",
    "\n",
    "for bar, val in zip(bars, values):\n",
    "    ax_bar.text(val + 1, bar.get_y() + bar.get_height()/2, f'{val:.1f}%',\n",
    "               va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax_bar.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{PROJECT_ROOT}/results/benchmark_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save results JSON\n",
    "results_data = {\n",
    "    'model': 'SAB-BYON-OMNI-v2.0',\n",
    "    'parameters': sum(p.numel() for p in model.parameters()),\n",
    "    'scores': scores,\n",
    "    'weights': weights,\n",
    "    'composite_score': weighted_total,\n",
    "    'grade': grade,\n",
    "    'perplexity_details': {\n",
    "        'overall_ppl': overall_ppl,\n",
    "        'median_ppl': median_ppl,\n",
    "    },\n",
    "    'coherence_details': coherence_details,\n",
    "    'speed_results': {str(k): v for k, v in speed_results.items()},\n",
    "}\n",
    "\n",
    "with open(f'{PROJECT_ROOT}/results/benchmark_results.json', 'w') as f:\n",
    "    json.dump(results_data, f, indent=2, default=str)\n",
    "\n",
    "print(f'\\nResults saved to {PROJECT_ROOT}/results/')\n",
    "print(f'  benchmark_results.png')\n",
    "print(f'  benchmark_results.json')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}